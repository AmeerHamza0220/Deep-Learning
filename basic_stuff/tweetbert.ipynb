{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#for downloading dataset\nfrom google.colab import files\n\nfiles.upload()\n! pip install -q kaggle\n! mkdir ~/.kaggle\n!cp kaggle.json ~/.kaggle/\n!kaggle datasets download -d datatattle/covid-19-nlp-text-classification","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers datasets\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n#latin encoding\ntrain=pd.read_csv('/content/Corona_NLP_train.csv',encoding='latin-1')\ntest=pd.read_csv(\"/content/Corona_NLP_test.csv\",encoding='latin-1')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenize train and test\nx_train=train['OriginalTweet'].values\nx_test=test['OriginalTweet'].values\ny_train=train['Sentiment'].values\ny_test=test['Sentiment'].values\n#tokenize train and test\ntr_tok=tokenizer(list(x_train),return_tensors='tf', truncation=True, padding=True, max_length=128)\nte_tok=tokenizer(list(x_test),return_tensors='tf', truncation=True, padding=True, max_length=128)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_datasets as tfds\nimport tensorflow_text as text  # A dependency of the preprocessing model\nimport tensorflow_addons as tfa\nfrom official.nlp import optimization\nimport numpy as np\n\ntf.get_logger().setLevel('ERROR')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"]=\"UNCOMPRESSED\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nif os.environ['COLAB_TPU_ADDR']:\n  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n  tf.config.experimental_connect_to_cluster(cluster_resolver)\n  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n  strategy = tf.distribute.TPUStrategy(cluster_resolver)\n  print('Using TPU')\nelif tf.config.list_physical_devices('GPU'):\n  strategy = tf.distribute.MirroredStrategy()\n  print('Using GPU')\nelse:\n  raise ValueError('Running on CPU is not recommended.')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Choose a BERT model to fine-tune\n\nbert_model_name = \"small_bert/bert_en_uncased_L-2_H-256_A-4\"\n #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_uncased_L-24_H-1024_A-16\", \"bert_en_wwm_uncased_L-24_H-1024_A-16\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_en_cased_L-24_H-1024_A-16\", \"bert_en_wwm_cased_L-24_H-1024_A-16\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"albert_en_large\", \"albert_en_xlarge\", \"albert_en_xxlarge\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\", \"talking-heads_large\"]\n\nmap_name_to_handle = {\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n    'bert_en_uncased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/3',\n    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_wwm_uncased_L-24_H-1024_A-16/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n    'bert_en_cased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_cased_L-24_H-1024_A-16/3',\n    'bert_en_wwm_cased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_wwm_cased_L-24_H-1024_A-16/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_base/2',\n    'albert_en_large':\n        'https://tfhub.dev/tensorflow/albert_en_large/2',\n    'albert_en_xlarge':\n        'https://tfhub.dev/tensorflow/albert_en_xlarge/2',\n    'albert_en_xxlarge':\n        'https://tfhub.dev/tensorflow/albert_en_xxlarge/2',\n    'electra_small':\n        'https://tfhub.dev/google/electra_small/2',\n    'electra_base':\n        'https://tfhub.dev/google/electra_base/2',\n    'experts_pubmed':\n        'https://tfhub.dev/google/experts/bert/pubmed/2',\n    'experts_wiki_books':\n        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n    'talking-heads_large':\n        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_large/1',\n}\n\nmap_model_to_preprocess = {\n    'bert_en_uncased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_en_wwm_cased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n    'bert_en_cased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n    'bert_en_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n    'bert_en_wwm_uncased_L-24_H-1024_A-16':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'bert_multi_cased_L-12_H-768_A-12':\n        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n    'albert_en_base':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n    'albert_en_large':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n    'albert_en_xlarge':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n    'albert_en_xxlarge':\n        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n    'electra_small':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'electra_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_pubmed':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'experts_wiki_books':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'talking-heads_base':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n    'talking-heads_large':\n        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n}\n\ntfhub_handle_encoder = map_name_to_handle[bert_model_name]\ntfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n\nprint('BERT model selected           :', tfhub_handle_encoder)\nprint('Preprocessing model auto-selected:', tfhub_handle_preprocess)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text as text\n\nfrom official.nlp import optimization  # to create AdamW optimizer\n\n\nclass Model():\n  def __init__(self):\n    pass  \n\n  def load_saved_model(self, path):\n    '''\n    Arguments: \n          path to saved model\n\n    '''\n    self.model = tf.keras.models.load_model(path)\n  \n  def load_model(self,model_name,model_url,preprocessor_url,num_classes=2):\n    '''\n     Load some bert model for fine tuning\n     Arguments:\n        model_url: url to bert model\n        model_name: name of the model\n        preprocess_url: url to preprocess model\n     \n    '''\n    self.model_url = model_url\n    self.model_name = model_name\n    self.preprocessor_url = preprocessor_url\n    self.bert_preprocess_model = hub.KerasLayer(self.preprocessor_url)\n    self.bert_model = hub.KerasLayer(self.model_url)\n    self.model = self.build_model(num_classes)\n\n  def build_model(self,num_classes):\n    ''' \n     build model for text classification using bert model\n    '''\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n    preprocessing_layer = hub.KerasLayer(self.preprocessor_url, name='preprocessing')\n    encoder_inputs = preprocessing_layer(text_input)\n    encoder = hub.KerasLayer(self.model_url, trainable=True, name='BERT_encoder')\n    outputs = encoder(encoder_inputs)\n    net = outputs['pooled_output']\n    net = tf.keras.layers.Dropout(0.1)(net)\n    net = tf.keras.layers.Dense(num_classes, activation='softmax', name='classifier')(net)\n    return tf.keras.Model(text_input, net)\n\n  def predict(self,input):\n    '''\n     Arguments:\n         input: input as string or list of strings\n     Returns:\n         predictions: list of prediction as tuple of (0/1 ,score)\n    '''\n    model_predictions = self.model.predict(tf.constant(input))\n    predictions = []\n    for i in range(input):\n      predictions.append((1 if model_predictions[i]>0.5 else 0,model_predictions[i]))\n    return predictions\n\n  def train(self,train_dataset,val_dataset,loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n      metrics = tf.metrics.BinaryAccuracy(),epochs = 10,\n      init_lr = 3e-5):\n   '''\n     Train the model on the data\n     Arguments:\n          train_dataset: training dataset\n          val_dataset: validation dataset\n          loss: loss function (default binary cross entropy)\n          metrics: metrics to be used (default binary accuracy)\n          epochs: number of epochs (default 5)\n          init_lr: initial learning rate  (default 3e-5)\n   '''\n   steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n   num_train_steps = steps_per_epoch * epochs\n\n   num_warmup_steps = int(0.1*num_train_steps)\n   optimizer = optimization.create_optimizer(init_lr=init_lr,\n                                          num_train_steps=num_train_steps,\n                                          num_warmup_steps=num_warmup_steps,\n                                          optimizer_type='adamw')\n                                          \n   self.model.compile(optimizer=optimizer,\n                         loss=loss,\n                         metrics=metrics)\n   self.model.fit(x=train_dataset,validation_data=val_dataset,\n                               epochs=epochs)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n#read  /content/Corona_NLP_train.csv\n\ntrain=pd.read_csv('/content/Corona_NLP_train.csv', encoding='latin-1')\ntest=pd.read_csv('/content/Corona_NLP_test.csv', encoding='latin-1')\ntrain","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10,6))\nplt.pie(train['Sentiment'].value_counts(), labels=train['Sentiment'].unique(), autopct='%.1f%%', textprops={'color':\"w\"})\nplt.legend(loc='upper right')\nplt.axis('equal')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#assign numbers to Sentiment labels\ntrain['Sentiment'] = train['Sentiment'].replace(['Negative','Neutral','Positive','Extremely Negative','Extremely Positive'],[1,0,2,3,4])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['Sentiment'] = test['Sentiment'].replace(['Negative','Neutral','Positive','Extremely Negative','Extremely Positive'],[1,0,2,3,4])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#convert to tensorflow dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((train['OriginalTweet'].values, train['Sentiment'].values)).batch(32)\ntest_dataset = tf.data.Dataset.from_tensor_slices((test['OriginalTweet'].values, test['Sentiment'].values)).batch(32)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#load bert model\nbert_model = Model()\nbert_model.load_model(bert_model_name,tfhub_handle_encoder,tfhub_handle_preprocess,num_classes=5)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_model.model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#loss sparse categorical cross entropy\nloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric=tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nbert_model.train(train_dataset,test_dataset,loss,metric)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now try using BertTweet ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModel, AutoTokenizer,TFAutoModelForSequenceClassification ,AutoConfig\n\n\nconfig = AutoConfig.from_pretrained(\"vinai/bertweet-base\",num_labels=5)\nberttweet = TFAutoModelForSequenceClassification.from_config(config)\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\",num_labels=5)\n\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenize train and test\nx_train=train['OriginalTweet'].values\nx_test=test['OriginalTweet'].values\ny_train=train['Sentiment'].values\ny_test=test['Sentiment'].values\n#tokenize train and test\ntr_tok=tokenizer(list(x_train),return_tensors='tf', truncation=True, padding=True, max_length=128)\nte_tok=tokenizer(list(x_test),return_tensors='tf', truncation=True, padding=True, max_length=128)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tr_tok.keys()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#select attention_mask and input_ids and token_type_ids\ntr_tok_ids=tr_tok['input_ids']\nte_tok_ids=te_tok['input_ids']\n\ntr_att_mask=tr_tok['attention_mask']\nte_att_mask=te_tok['attention_mask']\ntr_tok_type=tr_tok['token_type_ids']\nte_tok_type=te_tok['token_type_ids']\n#concate attention_mask and input_ids and token_type_ids\ntr_tok_train=tf.concat([tr_att_mask,tr_tok_ids,tr_tok_type],axis=-1)\nte_tok_test=tf.concat([te_att_mask,te_tok_ids,te_tok_type],axis=-1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n#map to number\ntrain['Sentiment'] = train['Sentiment'].replace(['Negative','Neutral','Positive','Extremely Negative','Extremely Positive'],[1,0,2,3,4])\ntest['Sentiment'] = test['Sentiment'].replace(['Negative','Neutral','Positive','Extremely Negative','Extremely Positive'],[1,0,2,3,4])\ny_train=train['Sentiment'].values\ny_test=test['Sentiment'].values\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=tf.data.Dataset.from_tensor_slices((tr_tok_ids,tr_att_mask,y_train)).batch(5)\ntest_data=tf.data.Dataset.from_tensor_slices((te_tok_test,y_test)).batch(5)\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmetric=tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\nberttweet.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),loss=loss,metrics=metric)\nberttweet.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import train","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"berttweet.fit(x=train_data,validation_data=test_data,epochs=2,verbose=1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids = tf.keras.Input(shape=(128,), dtype='int32')\nattention_mask = tf.keras.Input(shape=(128, ), dtype='int32')\n\nencoded = berttweet({\"input_ids\": input_ids, \"attention_mask\": attention_mask})\nlogits = encoded[0]\n \nmodel = tf.keras.models.Model(inputs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}, outputs = logits)\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]}]}